This lesson is about Q-learning. Recall that Q-learning is a model-free approach, meaning that it does not know about or use models of the transitions T or the rewards R. Instead, Q-learning builds a table of utility values as the agent interacts with the world. These Q-values can be used at each step to select the best action based on what it has learned so far. The fantastic thing about Q-learning is that it is guaranteed to provide an optimal policy. There is a hitch, however, that we'll cover later. Q learning is named after the Q function. What is that? Well, let's dig in and find out. Q can be written as a function, so we might have parentheses around s and a, or you can think of it as a table. So in this class we're going to view Q as a table. And it's got two dimensions, s and a. So s is the state that we're looking at, and a is the action we might take. Q represents the value of taking action a in state s, and there's two components to that. The two components are the immediate reward that you get for taking action a in state s, plus the discounted reward. And what that is about, what the discounted reward is about, is the reward you get for future actions. So an important thing to know is that Q is not greedy, in the sense that it just represents the reward you get for acting now. It also represents the reward you get for acting in the future. Adjusted close is not a good factor for learning, because you're not able to generalize over different price regimes for when the stock was low to when it was high. Also, if you're trying to learn a model for several stocks at once and they each hold very different prices, adjusted close doesn't serve well to help you generalize. Same thing is true for simple moving average. However if you combine adjusted close and simple moving average together into a ratio that makes a good factor to use in state. Bollinger Band value is also good. P/E ratio is good. And a new kind of feature that we're considering here with reinforcement learning, is whether we're holding the stock or not. That's an important state to know, in other words, if you're holding the stock it may be advantageous to get rid of it. But if you're not holding it, you might not necessarily want to sell. So this additional feature about what your situation is is useful. Similarly, return since we entered the position might be useful. In other words, this might help us set exit points for instance, maybe we've made 10% on the stock since we bought it and we should take our winnings while we can. So both of these are important and good factors to have an acute learning system. So an important consideration in what we're doing here is that our state is a single number. It's an integer. That way we can address it in our cue table. It is certainly the case that some reinforcement learning methods are able to treat the state as a continuous value. But just to get started here, let's use state as an integer so we can work more easily with the data. Now we have to do a little bit of work to get our state to an integer and here is the general way to do it. Our first step is to discretize each factor. It's a weird word, I'll explain it in a moment. But it essentially means convert that real number into an integer. Next is combine all of those integers together into a single number. We're assuming that we're using a discrete state space and that means more or less that our overall state is going to be this one integer that represents at once all of our factors. So consider that we have four factors and each one is a real number. Now we have separately beforehand figured out how to discretize each one of these factors. So we run each of these factors through their individual discretizers and we get an integer. Now I've happened to select integers between 0 and 9 but you can have larger ranges, for instance 0 to 20 or 0 to 100 even. It's easy if we just go from 0 to 9 because then we can stack them together into a number. But it's not too hard to think of algorithms will enable you to combine larger ranges of integers together. In this case we're just able to stack them one after the other into our overall discretized state. Discretization or discretizing is an important step in this whole process. What we want to do is have a way to convert a real number into an integer across a limited scale. Okay, let's recap what we've learned about Q-Learning. First, let's consider what it takes to build a model. First step is define states, actions, and rewards. So states are combinations of our features. Actions are buy, sell, do nothing. And rewards are some type of return. Next you choose the training period and you iterate over that training period and update your Q-table on each iteration. When you reach the end of that training period you backtest to see how good the model is and you go back and repeat, until the model quits getting better. Once it's converged you stop, you've got your model. Testing the model is simple you just backtest it on later data. Remember we're always training on one set of data and then testing on later data. And that's it.