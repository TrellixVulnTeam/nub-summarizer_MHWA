One problem with Q learning is that it takes many experienced tuples to converge. This is expensive in terms of interacting with the world, because you have to take a real step, in other words execute a trade, in order to gather information. To address this problem, Rich Sutton invented Diana. Diana works by building models of T, the transition matrix and R the reward matrix. Then after each real interaction with the world, we hallucinate many additional interactions, usually a few hundred. That are used then to update the queue table. Dyna-Q is an algorithm developed by Rich Sutton intended to speed up learning or model convergence for Q learning. Remember that Q learning is model free. Meaning that it does not rely on T or R. T being, our transition matrix and R being our reward function, it doesn't know it. Q learning does not know T or R. Dyna ends up becoming a blend of model free and model based methods. Okay, consider that state s and a are somewhat fixed. What we're trying to find is given s and a, what's the probability of a particular s prime? So, we'll begin this by just consulting how many times did this transition occur? In other words, how many times when we were in state s did action a, did we end up in s'? So that's how many times this particular transition occurred. Now we just need to divide it by the total number of times we were in state s and did action a. So essentially, it's the sum over i. Where we have i iterate over all the possible states of T[s, a, i]. So this is the number of times in total that we were in state s and executed action a. And so this ratio ends up being the probability that will end up in s' if we're in state s and take action a. The last step here is how do we learn a model for R? Now remember, when we execute an action a in state s, we get an immediate reward, little r. So again, big R, s, a as are expected reward if we're in state s and we execute action a. Little r is our immediate reward when we experience this in the real world. So big R is our model, little r is what we get in an experience tuple. So we want to update this model every time we have a real experience. And it's a simple equation, very much like the q table update equation. What we have is one minus alpha where alpha is our learning rate and again that can typically be something like zero point two. Anyways we multiply that times our current value for R and then we add in of course our new estimate of what that value ought to be. And we just use r for the new estimate. So it's alpha times little r, which is our immediate reward, or our new best estimate of what the value should be, plus what the value was before times 1 minus alpha. So we're waiting presumably. Our old value more than our new value, so we converge more slowly. But that's it. That's a simple way to build a model of R from observations of interactions with the real world. Before we close, let's recap really quick how Dyna-Q works. So, we start with straight, regular Q-Learning here and then we add three new components. The three components are, we update models of T and R, then we hallucinate an experience. And update our Q table. Now we may repeat this many times, in fact maybe hundreds of times, until we're happy. Usually, it's 1 or 200 here. Once we've completed those, we then return back up to the top and continue our interaction with the real world. The reason Dyna-Q is useful is that These experiences with the real world are potentially very expensive and these hallucinations can be very cheap. And when we iterate doing many of them, we update our Q table much more quickly.