Up until this point, we've focused on learners that provide forecast price changes. We then buy or sell the stocks with the most significant predicted price change. This approach ignores some important issues, such as the certainty of the price change. It also doesn't help us know when to exit the position either. In this lesson, we'll look at reinforcement learning. Reinforcement learners create policies that provide specific direction on which action to take. It's important to point out that when we say reinforcement learning, we're really describing a problem, not a solution. In the same way that linear regression is one solution to the supervised regression problem, there are many algorithms that solve the RL problem. Because I started out as a roboticists, I'm going to first explain this in terms of a problem for a robot. So here's our robot here and our robot is going to interact with the environment. So we represent the environment as this sort of cloud up here. So the robots going to take actions that'll change the environment. It will sense the environment, reason over what it sees and take another action. Finally, an important component of Markov decision problems is the reward function. And that's what gives us the reward. If we're in a particular state and we take an action A, we get a particular reward. So if we have all of these things defined, we have what's called a Markov decision problem. Now, the problem for a reinforcement learning algorithm is to find this policy pi that will maximize reward over time. And, in fact, if it finds the optimal policy, we give it a little symbol pi starred to indicate that it's optimal. With the infinite horizon what we're trying to maximize is the sum of all rewards over all of the future. So it's the sum of each of these rewards for i equals one to infinity. The finite horizon is very similar, it's just we don't go to infinity. So for optimizing over horizon of four steps, n would be four. We're just trying to maximize the sum of the reward for the next four steps. Now, there is yet another formulation that if you think back to that lecture a while back about what's the value of a future dollar. We can dig that up and it makes a lot of sense in terms of reinforcement learning. So remember that if it takes us say, four years to get a dollar, that dollar is less valuable than say if it takes one year. And the same way, if it takes, say, eight steps to make a dollar, that dollar is less valuable than a dollar I can get just in one step. And the way we represent that is very simple. Just like we represented the sum of future dividends and it looks like this, it's called discounted reward. So instead of just summing up the r sub i is we multiply it by this factor gamma to the i minus 1, such that our immediate reward, the very next one we get, whatever gamma is when it gets raised to the zero power is just one. So that means for the very next step we get r. But for the step after it, it's gamma to the one. So it devalues that reward a little bit. Gamma is a value between zero and one, the closer it is to one, the more we value rewards in the future. The closer it is to zero, the less we value rewards in the future. In fact, if gamma is set equal to one, this is exactly the same as the infinite horizon. But gamma relates very strongly to interest rates if you recall. So, if say, gamma were 0.95 it means each step in the future is worth about 5% less than the immediate reward if we got it right away. This is the method that we use in q learning. Let's summarize things and wrap up this lecture. I just want to repeat the points so you so reinforcement learning is something that we can use in trading. The problem for reinforcement learning algorithms is a Markov decision problem. And reinforcement learning algorithms solve them. A Markov decision problem is defined by S, A, T, and R, where S is the potential states, A are the potential actions, T is a transition probability, which is given I'm in state s, I take action a, what's the probability I'll end up in state S', and R is the reward function. The goal for reinforcement learning algorithm is to find a policy, pi, that maps a state to an action that we should take, and its goal is to find this pi such that it maximizes some future sum of the reward. We talked about that being either infinite horizon, fixed horizon, or discounted sum. We can map our task for trading to reinforcement learning and it works out like this. S, our states, are features about stocks and whether or not we're holding a stock. Actions are buy, sell, or do nothing. The transition function here is the market, and finally, the reward function is how much money we get at the end of a trade. So, we can apply reinforcement learning algorithms to find this policy. We've mentioned a few of those algorithms, for example policy iteration, and value iteration, and Q learning, but we haven't talked in detail what they are, and that's the subject of lessons coming up.