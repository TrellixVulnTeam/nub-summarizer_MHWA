In 1988, Michael Kearns and Leslie Valiant posed the following question. Can a set of weak learners be combined to create a single, strong learner? One answer to that question came in 2009. Back in 2006, Netflix offered a $1 million prize for a machine learning algorithm that could do 10% better than their own algorithm at predicting which movies their customers would like to see. The prize was not awarded until three years later in 2009. The winning algorithm was not a single algorithm, but a combination of several, or an ensemble. This lesson is about ensemble learners. Creating an ensemble of learners is one way to make the learners you've got better. So we're not talking about creating a new algorithm, but instead assembling together several different algorithms or several different models to create an ensemble learner. One thing I want to emphasize here is that you can take what you learn here about ensemble learners and plug it right in to what you're already doing with your KNN and linear regression models. Now, what we've been doing so far, is that we've had one kind of learning method, say KNN, we plug our data into there and we learn a model. We can query our model with an X and it will give us a Y. So this is not an ensemble learner, this is just a single learner. And the idea with ensemble learners is that we have several additional learners. So, we might have a linear regression based model, we might have a decision tree based model, we might have a support vector machine based model. You could continue this on with any different number of algorithms. They're all trained using the same data, and so now we have, in this case, four different models. To query this ensemble of learners, we query each model by itself and combine the answers. So if we wanted to query this model with X, we plug X into each model, the same X and then our Ys come out. So we have a Y output from each of these models, how do we combine them? If we're doing classification where for instance we're trying to identify what the thing is, we might have each of these Ys vote on what it is. But we're doing regression, and so the typical thing to do here is to take the mean, and that is the result for this ensemble learner. We can then test this overall ensemble learner using this test data that we set aside. Why ensembles? Why do we use them, why might they be better? Well, there's a few reasons. First of all, ensembles often have lower error than any individual method by themselves. Ensemble learners offer less overfitting. The ensemble of learners typically does not overfit as much as any individual learner by itself. Now why is that? Here's at least an intuitive answer. As each kind of learner that you might use has a sort of bias, it's easiest to talk about that in terms of linear regression in terms of what do I mean by bias. So clearly, with linear regression our bias is that the data is linear. KNN has its own kind of bias, decision trees have their own kind of bias, but when you put them together you tend to reduce the biases because they're fighting against each other in some sort of way. Anyways that's what an ensemble learner is like if we use multiple types of learners. We select randomly from our training data. We then train a model in a usual way. The next thing we do, and this is something different, we take all our training data and use it to test the model in order to discover that some of the points in here, our x's and our y's, are not well predicted. So there's going to be some points in here for which there is significant error. Now, when we go to build our next bag of data, again, we choose randomly from our original data. But each instance is weighted according to this error. So, these points that had significant error, are more likely to get picked and to go into this bag than any other individual instance. So as you see, we ended up with a few of those points in here and a smattering of all the other ones as well. We build a model from this data and then we test it. Now we test our system altogether. In other words, we've got a sort of miniature ensemble here, just two learners. And we test both of them. We test them by inputting again this in-sample data. We test on each instance and we combine their outputs. And again we measure error across all this data. Maybe this time these points got modeled better, but there were some other ones up here that weren't as good. And thus we build our next bag and our next model. And we just continue this over, and over and over again up until m or the total number of bags we'll be using. So to recap, bagging, when we build one of these instances, is simply choosing some subset of the data at random with replacement, and we create each bag in the same way. Boosting is an add-on to this idea where in subsequent bags we choose those data instances that had been modeled poorly in the overall system before. All right, so I want you to think back over what we've been talking about, bagging and add a boost. Which is more likely to overfit as m increases? Now keep in mind that m is the number of bags that we're using, or the number of models that we're building to create our ensemble. So as m increases, which one is more likely to overfit? The answer is Ada Boost and the reason is that Ada Boost is trying really really hard to match those parts of the data that are off or outliers or whatever, and accordingly it's striving to fit, and subsequently it may be susceptible to over fitting. Before we finish this lesson, I wanted to summarize things and tell you how this all fits in to machine learning for trading. The first thing to point out here is that bagging and boosting are just methods for taking existing learners and essentially wrapping them in this meta algorithm that converts your existing learner into an ensemble. And you should use the same API to call your ensemble that you would have earlier been using to call an individual learner. So externally, to whatever part of your program is calling the learner, it doesn't know that underneath there you're doing boosting or bagging. Your resulting learner is also likely to lower error and reduced overfitage. So to summarize, boosting and bagging are not new algorithms in and of themselves. They're meta algorithms that let you wrap your underlying learning algorithms into something that's better.