We've posed the general problem of supervised regression learning and introduced two algorithms that can solve it. Linear regression creates parametrize models and K and N is a non-parametric instance base method. There are in fact many algorithms that can solve this problem. Each algorithm has it's on pros and cons. In this lesson we'll look at various methods for assessing those algorithms. As we begin now looking at how to evaluate various machine learning algorithms, let's start back with KNN and look a little more closely at the sorts of solutions it provides. Let's start with our training data, and remember we've got pairs of X and Y, so each one of these dots represents one training tuple. And I'm just making this data up, of course. But suppose we were going to query this KNN model over in this region. Say right here at this point. Well, the nearest three. Let's use K=3 here. The nearest three are going to be these. And remember, we take the mean of their value to get the value at that query point. So if we query from here all the way to about here, our model is going to take the mean y value of those, so the output of our model is going to look something like this. And notice it gives the same value at all these points. Eventually, as we query from left to right, we get to a point where this one gets dropped out, and this one gets added in. And at that point we'll have a sudden drop about like that in the model. And we continue on like this. We'll have another drop like that. If we query our model now from left to right in very, very tiny increments we'll get the result that looks something like this. Note that indeed there are sort of jump points here. Some nice things about this are that it's not over fitting the data. In other words, it's not tagging each point. A negative aspect though is at the ends there we have these horizontal lines that are no longer changing or essentially this model is not able to extrapolate like we might if we had a parametric model. Let's consider now what happens to the model that comes out when we change the value of k. So we've got three k nearest neighbor models here. Each one is using a different value of k, and I want you to match the value of k to the output model here. Okay, so I want you to look at these different charts. Each one of these models shown in red is using a different value of k. So I want you to fill in these little boxes, which chart corresponds to the value of k. So one of these charts was created with k=1, one was created with k=3, and another was created with k=N, where N is the total number of elements in the dataset. And there's another question I want you to answer. True or false, as we increase k we are more likely to overfit. And in fact, it can be shown with a polynomial like this that as the order of the polynomial or d reaches in, the total number of points, we actually can match the data at every point. Now a couple things to note here. One is as we go off the edge here for all these models, we're able to extrapolate in the direction the data seem to be going. And this is capability that parametric models or these polynomial models have that KNN does not. I've shown you some graphs that suggest the ways the models can fit the data, more or less closely. But let's have a more formal definition of this matching. It's called error. A standard way to measure error, is called RMS error. Let me show you how to calculate this. Let's suppose we use this data, which are these green points, to build a model. Let's say it's a linear model like this. We can assess the model at each real data point. For instance, at this data point. And measure the difference between the Y value of the data point, and the model. And this difference is error. Now, we've got an error at every single one of these data points. And what we do to measure root mean squared error, is to take the error at each one of these points, square it, add them together, take the average, and take the square root of that. So that sounds kind of complicated, but here's what it looks like. Ytest minus Ypredict. So Ytest are the actual values of the data. Ypredict are what our model predicted. We take that difference at each point. That's this difference. Square it, sum all those together, divide by the number of points and take the square root. And that's our root mean squared error. And what this is an approximation of really, is sort of the average error here. But we end up emphasizing larger errors a bit more. Now, we just measured the error of this linear model against our original training data. We know, though, from say, k and n, that we can build models that can fit this training data exactly. So we can have arbitrarily small error against our training set. The more important measure is, what is our error out of sample? So, what out of sample means is we train on our training set, but we test on a separate testing set of data. And, that's going to be different than our training set. So, to measure out of sample error, we look at the error from our testing set, not our training set. So we look at each one of these test points and measure the error for each one of those. So we look at these blue points instead of the green points, plug them into this equation just like before, and that's our out of sample root mean squared error. I want you to think about each one of these and select which you think has better performance in that regard, linear regression or KNN. So let's step through them. How much memory do you need in your computer to save the model? How much compute time do you need to train the model? How long does it take to query the model? And finally, how easy is it to add new data to your model? So, again, I want you to check the box according to which one has better performance with regard to these factors. So, in terms of space for saving the model, linear regression is a hands down winner. For instance, if we're learning a third order polynomial, we have to only store four numbers. KNN, on the other hand, requires you to keep all the data, so it could be megabytes or gigabytes of data. So, KNN is bad in this regard. Compute time to train. KNN is much better in this case. In fact, it takes zero time to train KNN. You just stuff the model into a data store and you're done. On the other hand, linear regression has to take all that data, compute over it, to find those parameters. Compute time to query. LinReg wins hands down. All you do is you plug your X in, multiply it out and that's the answer. KNN requires quite a bit of time to query because you have to, among other things, usually do a sort to cross all the data. Ease to add new data. KNN wins that because all you gotta do is just plop it in there, you don't have to do any re-calculation. With linear regression, you have to add the new data and then recompute the factors. Well, that's all for how to assess learning algorithms.