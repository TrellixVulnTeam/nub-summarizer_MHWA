{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "fine-tuning T-5 on CNN+daily mail + ML4T.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPjfWuvDLnWC6L8ur9Eh8SQ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sbassam/nub-summarizer/blob/master/fine_tuning_T_5_on_CNN%2Bdaily_mail_%2B_ML4T.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8eKO_znIpplT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KKcfHbJnO6kz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "cd /content/drive/My Drive/summarizer/nub-training-evaluation"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RGp2GGMp64AT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# the one-time data download for the first run\n",
        "# !wget https://s3.amazonaws.com/datasets.huggingface.co/summarization/cnn_dm.tgz\n",
        "# !tar -xzvf cnn_dm.tgz\n",
        "\n",
        "# !export CNN_DIR=${PWD}/cnn_dm"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LIUXcd2FPIbC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install transformers -q\n",
        "!pip install wandb -q\n",
        "!pip install rouge-score"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qMvG9b6APsGj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import shutil\n",
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader, RandomSampler, SequentialSampler\n",
        "from transformers import T5Tokenizer, T5ForConditionalGeneration, pipeline\n",
        "import wandb\n",
        "from torch import cuda"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NvhROlBpP9h-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "device = 'cuda' if cuda.is_available() else 'cpu'\n",
        "!nvidia-smi"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rJf57PntQFtj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!wandb login"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v63anqWMLLxf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "with open('cnn_dm/train.source') as f:\n",
        "    train_source = f.readlines()\n",
        "train_source = [x.strip() for x in train_source] \n",
        "train_source = pd.DataFrame(train_source)\n",
        "with open('cnn_dm/train.target') as f:\n",
        "    train_target = f.readlines()\n",
        "train_target = [x.strip() for x in train_target] \n",
        "train_target = pd.DataFrame(train_target)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KcAGgs0PI6tC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "TRAIN_SIZE = #insert train size"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "clSCb0ACQbdW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sample_ids = np.random.choice(train_source.size, replace = False, size = TRAIN_SIZE) \n",
        "cnn_dm_train = pd.concat([train_source.iloc[sample_ids], train_target.iloc[sample_ids]], axis=1)\n",
        "cnn_dm_train.columns = ['full_text', 'summary']\n",
        "cnn_dm_train.full_text = 'summarize: ' + cnn_dm_train.full_text"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qmgLHqYPtxLW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df_lessons = pd.read_csv('/content/drive/My Drive/summarizer/nub-training-evaluation/lesson_summary.csv')\n",
        "df_lessons = df_lessons[['summary','full_text']]\n",
        "df_lessons.full_text = 'summarize: ' + df_lessons.full_text"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b9WGpWQvuI30",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "cnn_dm_train = pd.concat([cnn_dm_train, df_lessons], axis = 0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qmTCxoQ1uO3a",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "cnn_dm_train.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cB7KrZFUQIP8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# class and functions from  github.com/abhimishra91/transformers-tutorials\n",
        "\n",
        "\n",
        "class CustomDataset(Dataset):\n",
        "\n",
        "    def __init__(self, dataframe, tokenizer, source_len, summ_len):\n",
        "        self.tokenizer = tokenizer\n",
        "        self.data = dataframe\n",
        "        self.source_len = source_len\n",
        "        self.summ_len = summ_len\n",
        "        self.summary = self.data.summary\n",
        "        self.full_text = self.data.full_text\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.summary)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        full_text = str(self.full_text[index])\n",
        "        full_text = ' '.join(full_text.split())\n",
        "\n",
        "        summary = str(self.summary[index])\n",
        "        summary = ' '.join(summary.split())\n",
        "\n",
        "        source = self.tokenizer.batch_encode_plus([full_text], max_length=self.source_len, pad_to_max_length=True, return_tensors='pt', truncation=True)\n",
        "        target = self.tokenizer.batch_encode_plus([summary], max_length=self.summ_len, pad_to_max_length=True, return_tensors='pt', truncation=True)\n",
        "\n",
        "        source_ids = source['input_ids'].squeeze()\n",
        "        source_mask = source['attention_mask'].squeeze()\n",
        "        target_ids = target['input_ids'].squeeze()\n",
        "        target_mask = target['attention_mask'].squeeze()\n",
        "\n",
        "        return {\n",
        "            'source_ids': source_ids.to(dtype=torch.long),\n",
        "            'source_mask': source_mask.to(dtype=torch.long),\n",
        "            'target_ids': target_ids.to(dtype=torch.long),\n",
        "            'target_ids_y': target_ids.to(dtype=torch.long)\n",
        "        }\n",
        "\n",
        "\n",
        "def train(epoch, tokenizer, model, device, loader, optimizer):\n",
        "    model.train()\n",
        "    for _, data in enumerate(loader, 0):\n",
        "        y = data['target_ids'].to(device, dtype=torch.long)\n",
        "        y_ids = y[:, :-1].contiguous()\n",
        "        lm_labels = y[:, 1:].clone().detach()\n",
        "        lm_labels[y[:, 1:] == tokenizer.pad_token_id] = -100\n",
        "        ids = data['source_ids'].to(device, dtype=torch.long)\n",
        "        mask = data['source_mask'].to(device, dtype=torch.long)\n",
        "\n",
        "        outputs = model(input_ids=ids, attention_mask=mask, decoder_input_ids=y_ids, lm_labels=lm_labels)\n",
        "        loss = outputs[0]\n",
        "\n",
        "        if _ % 10 == 0:\n",
        "            wandb.log({\"Training Loss\": loss.item()})\n",
        "\n",
        "        if _ % 500 == 0:\n",
        "            print(f'Epoch: {epoch}, Loss:  {loss.item()}')\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "\n",
        "\n",
        "def main(train_dataset, model_output_dir):\n",
        "    # WandB – Initialize a new run\n",
        "    wandb.init(project=\"transformers_tutorials_summarization\")\n",
        "\n",
        "    # WandB – Config is a variable that holds and saves hyperparameters and inputs\n",
        "    # Defining some key variables that will be used later on in the training\n",
        "    config = wandb.config  # Initialize config\n",
        "    config.TRAIN_BATCH_SIZE = 1  # input batch size for training (default: 64)\n",
        "    config.TRAIN_EPOCHS = 2  # number of epochs to train (default: 10)\n",
        "    config.LEARNING_RATE = 1e-4  # learning rate (default: 0.01)\n",
        "    config.SEED = 42  # random seed (default: 42)\n",
        "    config.MAX_LEN = 1024\n",
        "    config.SUMMARY_LEN = 256\n",
        "\n",
        "    # Set random seeds and deterministic pytorch for reproducibility\n",
        "    torch.manual_seed(config.SEED)  # pytorch random seed\n",
        "    np.random.seed(config.SEED)  # numpy random seed\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "\n",
        "    # tokenzier for encoding the text\n",
        "    tokenizer = T5Tokenizer.from_pretrained(\"t5-base\")\n",
        "\n",
        "    \n",
        "    # Creating the Training and Validation dataset for further creation of Dataloader\n",
        "    training_set = CustomDataset(train_dataset.reset_index(drop=True), tokenizer, config.MAX_LEN, config.SUMMARY_LEN)\n",
        "\n",
        "    # Defining the parameters for creation of dataloaders\n",
        "    train_params = {\n",
        "        'batch_size': config.TRAIN_BATCH_SIZE,\n",
        "        'shuffle': True,\n",
        "        'num_workers': 0\n",
        "    }\n",
        "\n",
        "    # Creation of Dataloaders for testing and validation. This will be used down for training and validation stage for the model.\n",
        "    training_loader = DataLoader(training_set, **train_params)\n",
        "\n",
        "    # Defining the model. We are using t5-base model and added a Language model layer on top for generation of Summary.\n",
        "    # Further this model is sent to device (GPU/TPU) for using the hardware.\n",
        "    model = T5ForConditionalGeneration.from_pretrained(\"t5-base\")\n",
        "    model = model.to(device)\n",
        "\n",
        "    # Defining the optimizer that will be used to tune the weights of the network in the training session.\n",
        "    optimizer = torch.optim.Adam(params=model.parameters(), lr=config.LEARNING_RATE)\n",
        "    wandb.watch(model, log=\"all\")\n",
        "\n",
        "    for epoch in range(config.TRAIN_EPOCHS):\n",
        "        train(epoch, tokenizer, model, device, training_loader, optimizer)\n",
        "\n",
        "    model_to_save = model.module if hasattr(model, 'module') else model\n",
        "    model_to_save.save_pretrained(model_output_dir)\n",
        "    tokenizer.save_pretrained(model_output_dir)\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q5gUk5_WBeSJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dir = './model'\n",
        "if os.path.exists(dir):\n",
        "    shutil.rmtree(dir)\n",
        "os.makedirs(dir)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iWFUn3muBekW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "main(cnn_dm_train, dir)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7zNsaCBSYhTQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# register the model in huggingface model hub\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}