from transformers import LongformerTokenizer, LongformerForQuestionAnswering
import torch
from os import listdir, walk
from os.path import isfile, join
import re
import argparse


# modified script based on  https://colab.research.google.com/drive/1zEl5D-DdkBKva-DdreVOmN0hrAfzKG1o?usp=sharing#scrollTo=zFWlfEJllAcw

def get_sent_id(input_id, max_startscore, max_endscore):
    sent_start_id = sent_end_id = 0
    all_tokens = tokenizer.convert_ids_to_tokens(input_id)
    for ind, token in enumerate(all_tokens):
        if (token == "." or token == "?") and ind < max_startscore:
            sent_start_id = ind + 1
        if (token == "." or token == "?") and ind > max_endscore:
            sent_end_id = ind
            break
    return sent_start_id, sent_end_id + 1


def generate_qa(question_context_batch, model, tokenizer):
    summary_tuple = []
    # # context = "There are two broad categories of approaches to use for choosing stocks to buy or sell. They're based on Fundamental analysis and Technical analysis. Fundamental analysis involves looking at aspects of a company in order to estimate its value. Fundamental investors typically look for situations where the price of a company is below its value. Another camp is based on Technical analysis. Technicians don't care about the value of a company. Instead, they look for patterns or trends in a stock's price. This lesson will focus on technical analysis.  Let's start by looking at a few characteristics of technical analysis. First, what is it? One of the most important things to remember about technical analysis is that it looks only at price and volume. That's as opposed to fundamental analysis that looks at fundamental factors, like earnings, dividends, cash flow, book value, and so on. Technical analysis is price and volume only. We look back at historical price and volume to compute statistics on this time series, and these statistics are called indicators. Indicators are heuristics that may hint at a buy or sell opportunity. Now there is significant criticism of the technical approach. Folks consider that it's not an appropriate method for investing, because it's not considering the value of the companies. Instead, maybe you could think about it as a trading approach as opposed to an investing approach. "
    # # context = "Up until this point, we've focused on learners that provide forecast price changes. We then buy or sell the stocks with the most significant predicted price change. This approach ignores some important issues, such as the certainty of the price change. It also doesn't help us know when to exit the position either. In this lesson, we'll look at reinforcement learning. Reinforcement learners create policies that provide specific direction on which action to take. It's important to point out that when we say reinforcement learning, we're really describing a problem, not a solution. In the same way that linear regression is one solution to the supervised regression problem, there are many algorithms that solve the RL problem. Because I started out as a roboticists, I'm going to first explain this in terms of a problem for a robot. So here's our robot here and our robot is going to interact with the environment. So we represent the environment as this sort of cloud up here. So the robots going to take actions that'll change the environment. It will sense the environment, reason over what it sees and take another action. In robotics, we call this the sense, think, act cycle and you don't have to implement it only using reinforcement learning. There's many ways that you could implement sense, think, act, but we're going to focus on how to do that with reinforcement learning. Okay, so our robot observes the environment and some form of description of the environment comes in. Let's call that the state s, so s is our letter that represents what we see in the environment. Now the robot has to process that state somehow to determine what to do. And we call this pi or policy, so the robot takes in the state s and then outputs an action. We'll call that action a and it affects the environment in some way and changes it. Now this is a sort of circular process, the action a is taken into the environment and the environment then transitions to a new state. So T is this transition function that takes in what its previous state was and the action and moves to a new state. And that new state comes out, boom, back into the robot. Robot looks at his policy, action comes out. Now there's a question, how do we arrive at this policy? How do we find pi? Well, that's what we're going to spend a couple lessons on, but this whole puzzle is missing a piece and that's the thing that helps us find pi. And part of that piece is well, there's this other part called r which is the reward. So everytime the robot is in a particular state and it takes an action. There's a particular reward associated with taking that action in that state and that reward comes into the robot. And you can think of the robot has having a little pocket where it keeps cash and that's what that reward is. And the robot's objective is, over time, to take actions that maximize this reward. And somewhere within the robot, there's an algorithm that takes all this information over time to figure out what that policy ought to be. So let me recap a little bit. S is the state of our environment and that's what the robot senses in order to decide what to do. It uses its policy pi to figure out what that action should be. And by the way, pi can be a simple look up table. Over time, each time the robot takes an action, it gets a reward and it's trying to find the pi that will maximize its reward over time. Now in terms of trading, our environment really is the market and our actions are actions we can take in the market, like buying and selling or holding. S are factors about our stocks that we might observe and know about. And r is the return we get for making the proper trades. Now as you know, we want to use reinforcement learning algorithms to trade with. So let's think now about how we can map the trading problem to reinforcement learning. Okay, so consider each of these factors. Buy, sell, holding long, Bollinger value, return from trade, daily return. And then consider, is that item a description of our state that we ought to consider before we make a trade? Is it an action that we give to the market to cause a trade to occur? Or is it a potential reward that we would use to inform our algorithm for learning how to trade. And it's potentially the case that some of these may serve more than one role. Okay, let's step through these one at a time. Buy and sell are actions. So, those are directives we give to the market or the environment to change it, and potentially change our state. Holding long is a part of the state, it tells us whether we are holding the stock or not. We might also be holding short if we had shorted of the stock. So holding long is a part of the state. Bollinger value, that's a feature, a factor that we can measure about a stock, and that's part of the state as well. That would inform us whether we wanted to act on it in some way with an action. Return from trade, when we finally exit a position. That is our reward. We might lose money, so it would be a negative reward if we lost money. We might make money and that'd be a positive reward, so that's R a reward. Daily return, that could be either a state, in other words a factor we consider for deciding what to do, but it could also be a reward, we'll get into that more later and you'll see how it could be one or the other. Let's consider now a little more carefully how we map trading to an RL problem. So first of all the environment here is really the market. Our state that we're going to consider includes things like market features, prices, whether we're holding the stock. I'll list a few of those items right here. Our actions are things like buy and sell, and potentially do nothing is also an allowable action. So let's think about this in the context of trying to learn how to trade a particular stock. So we've got this historical time series, and let's say this vertical line is today. Now we can look back over time to infer the state of the stock. So what are the Ballinger Band values and things like that. Now we process that and decide what's our action. Let's suppose that we decide to buy. So once we buy, we're now holding long. That's part of our state. We go forward, we're now on a new state where the price has gone up. We're holding long, lets suppose we decide to sell at that point. So we've had two actions, well we've been in two states in state one we were not holding. We executed the action by, went forward in time, we're holding along now, and then we execute the action sell. Note that we made money here and that's our reward, r. So just to recap for a moment. The policy that we learn tells us what to do at each time we evaluate state, and we're going to learn that we haven't talked yet about how we learned the policy. But we're going to learn the policy by looking at how we accrue money or don't based on the actions we take in the environment. Let's formalize this a little bit. What we've been working with is something called a Markov decision problem. And here's what makes up a Markov decision problem. There are a set of states S. Those are all the values that this S can take as it comes into the robot. There's a set of actions A, which is these potential actions we can take to act on the environment. There's a transition function. This is the T within the environment. And this is a little bit complicated, but let's just step through it. T is a three-dimensional object, and it records in each of its cells the probability that if we are in state S and we take action A, we will end up in state S prime. Something to note about this transition function is, suppose we're in state, a particular state S and we take a particular action A. The sum of all the next states we might end up in has to sum to one. In other words, with probability one, we're going to end up in some new state, but the distribution of probabilities across these different states is what makes this informative and revealing. Finally, an important component of Markov decision problems is the reward function. And that's what gives us the reward. If we're in a particular state and we take an action A, we get a particular reward. So if we have all of these things defined, we have what's called a Markov decision problem. Now, the problem for a reinforcement learning algorithm is to find this policy pi that will maximize reward over time. And, in fact, if it finds the optimal policy, we give it a little symbol pi starred to indicate that it's optimal. Now, if we have these, and, in particular, if we have T and R, there are algorithms we can unleash that will find this optimal policy. Two of them are policy iteration and value iteration. Now, in this class, we don't start off knowing T and R, and so we're not going to be able to use these algorithms directly to find this policy. Most of the time we don't have this transition function, and we don't have the reward function either. So the robot, or the trader, whatever environment we're in, has to interact with the world, observe what happens, and work with that data to try to build a policy. So let me give you an example here. Let's say we were in state S1. So, that's what we observed there. Our robot took action, A1. I'm making this little subscript to indicate which step in this series of steps it's at. We then find our self in S'. And we get reward R. Now this is an experience tubal. This is very similar to experience tubal in regression learning where we have an X and a Y paired together. That's an experience tubal of you know, when you observe this X you see this Y. Here we're saying when you observe the state, S1, you take action, A1, you end up in this new state, at least it's an example of you ending up in this new state S1', and reward, R1. Now we find ourselves in a new state S2, but that's really, this state is where we found our self. We take some new action, A2, we end up in some new state, S2', and we get a new reward, R2. When we do this over and over and over and over and over again, gathering experience tuples all along the way. Now, if we have this trail of experience tuples, there's two things we can do with them in order to find that policy pi. The first set of approaches is called model based reinforcement learning. What we do is we look at this data over time and we build a model of T just by looking statistically at these transitions. In other words we can look a every time we were in a particular state and took a particular action. And see which new states we ended up in. And just build a tabular representation of that. It's not hard. Similarly, we can build a model of R. We just look statistically when we're in a particular state, and we take an action, what's the reward? We can just average that over all these instances. Once we have these models, we can then use value iteration or policy iteration to solve the problem. There's another set of approaches called model-free. And that's the type we're going to focus on. In particular we're going to learn about Q-learning. And model-free methods develop a policy just directly by looking at the data. And of course we'll talk about those soon. We didn't go into enough detail about what it is we're trying to optimize here. I just said something vague like we want to maximize the sum of our reward. Well, it's not so simple, in fact, here's a great story to illustrate that. There's a great Russian comedian, Yakov Smirnoff, you may remember him or not, but he told this joke once that I really loved. He said, have you heard about the Soviet lottery, it's a million rubles if you win, one ruble a year for a million years. So the point is, and if you recall from one of our earlier lessons, that one dollar or one ruble delivered to us a million years in the future is really not as valuable as a dollar or ruble that we get now. And so, for instance, if we think about a robot living forever, it might do something just mundane to gather a dollar a year. That's an infinite amount of money, but in practice it doesn't really work that well. So to consider that, and to illustrate that, I'm going to show you a little maze problem here, and we'll think about what the robot ought to do that would be optimal in this maze. So here's our robot, and here's the challenge for our robot. We have a reward here of $1 and a reward over here of $1 million. So if the robot comes over here and gets this $1, it's special in that each time he touches it, he gets $1 and it goes away but then it comes back. So the robot could come here go back and forth and get a dollar each time it moves here. This one, once the robot tags it, it's gone. But clearly it's worthwhile to come over here and grab it. Now this red area is obstacle, it can't go there. And here I wrote some rewards that the robot, in fact negative one is a penalty. But the penalties the robot would get as it went this way, and zero penalty that way. Now, if we say that what we want to optimize is the sum of all future rewards, then it doesn't matter whether we go this way and just get that dollar over and over and over again. Or if we go this way, get the million dollars, come back and get that $1 over and over and over again. Now there's no difference because they both sum to infinity over time. Now what if we say, okay, I want to optimize my reward over three moves. So I've got a finite horizon. Let's consider the rewards we get with a finite horizon of three if we go this way versus this way. So if we go this way, we're going to get rewards of -1, -1, -1, and if we go this way we get zero, $1, and then we have to move down here, and get another zero. So clearly, starting here, with a finite horizon of three, the best thing to do is go up there. Now, if we extend the horizon a little bit further, say out to eight, we would find that this is the best thing to do. So if we go this way, we get -1, -1, -1, until we hit the jackpot here and get $1M. Clearly if you sum this up, it's a pretty good prize. If we go this way and touch that $1 over and over again, we get this. So clearly as we expand our finite horizon trivially up to say eight steps, going this way and tagging at one million is the best thing to do. If we carried it even further, we'd discover that then we should come back this way and go to that dollar and tag it over and over and over again. Let me formalize these a little bit. With the infinite horizon what we're trying to maximize is the sum of all rewards over all of the future. So it's the sum of each of these rewards for i equals one to infinity. The finite horizon is very similar, it's just we don't go to infinity. So for optimizing over horizon of four steps, n would be four. We're just trying to maximize the sum of the reward for the next four steps. Now, there is yet another formulation that if you think back to that lecture a while back about what's the value of a future dollar. We can dig that up and it makes a lot of sense in terms of reinforcement learning. So remember that if it takes us say, four years to get a dollar, that dollar is less valuable than say if it takes one year. And the same way, if it takes, say, eight steps to make a dollar, that dollar is less valuable than a dollar I can get just in one step. And the way we represent that is very simple. Just like we represented the sum of future dividends and it looks like this, it's called discounted reward. So instead of just summing up the r sub i is we multiply it by this factor gamma to the i minus 1, such that our immediate reward, the very next one we get, whatever gamma is when it gets raised to the zero power is just one. So that means for the very next step we get r. But for the step after it, it's gamma to the one. So it devalues that reward a little bit. Gamma is a value between zero and one, the closer it is to one, the more we value rewards in the future. The closer it is to zero, the less we value rewards in the future. In fact, if gamma is set equal to one, this is exactly the same as the infinite horizon. But gamma relates very strongly to interest rates if you recall. So, if say, gamma were 0.95 it means each step in the future is worth about 5% less than the immediate reward if we got it right away. This is the method that we use in q learning. One reason is that the math turns out to be very handy, and it provides nice conversion properties. I want you to consider each of these optimizations and answer which of those will get us to the $1 million. In other words, if the robot is trying to maximize the sum over these horizons, which ones will lead it to a policy that causes it to reach that $1 million? So there are actually several that satisfy that. Infinite horizon is a little bit iffy because the robot can go this way and get a dollar on every other move and that will add up to infinity. It can go here and get the $1 million and then come back and do that and it will add up to infinity. So it's possible that infinite horizon will cause it to do that but there's two equivalent solutions. Finite with n=4, no it won't get to that $1 million. Because if it tries to go that way, it'll only get negative reward here, but it'll get positive reward if it goes that way. However, if we let n go out to 10, boom, it'll reach that $1 million. And finally, discounted reward, where each dollar in the future is only worth 0.95, and that gets smaller as we get further and further into the future. Still, by the time we get to the eight steps that it takes to reach this reward It's still so huge that that's clearly the optimal thing to do. Okay, so those are the answers to which horizons will cause us to get to that $1 million. Let's summarize things and wrap up this lecture. I just want to repeat the points so you so reinforcement learning is something that we can use in trading. The problem for reinforcement learning algorithms is a Markov decision problem. And reinforcement learning algorithms solve them. A Markov decision problem is defined by S, A, T, and R, where S is the potential states, A are the potential actions, T is a transition probability, which is given I'm in state s, I take action a, what's the probability I'll end up in state S', and R is the reward function. The goal for reinforcement learning algorithm is to find a policy, pi, that maps a state to an action that we should take, and its goal is to find this pi such that it maximizes some future sum of the reward. We talked about that being either infinite horizon, fixed horizon, or discounted sum. We can map our task for trading to reinforcement learning and it works out like this. S, our states, are features about stocks and whether or not we're holding a stock. Actions are buy, sell, or do nothing. The transition function here is the market, and finally, the reward function is how much money we get at the end of a trade. So, we can apply reinforcement learning algorithms to find this policy. We've mentioned a few of those algorithms, for example policy iteration, and value iteration, and Q learning, but we haven't talked in detail what they are, and that's the subject of lessons coming up. Okay, that's it for reinforcement learning, I'll see you again soon."
    # context = "We've posed the general problem of supervised regression learning and introduced two algorithms that can solve it. Linear regression creates parametrize models and K and N is a non-parametric instance base method. There are in fact many algorithms that can solve this problem. Each algorithm has it's on pros and cons. In this lesson we'll look at various methods for assessing those algorithms. As we begin now looking at how to evaluate various machine learning algorithms, let's start back with KNN and look a little more closely at the sorts of solutions it provides. Let's start with our training data, and remember we've got pairs of X and Y, so each one of these dots represents one training tuple. And I'm just making this data up, of course. But suppose we were going to query this KNN model over in this region. Say right here at this point. Well, the nearest three. Let's use K=3 here. The nearest three are going to be these. And remember, we take the mean of their value to get the value at that query point. So if we query from here all the way to about here, our model is going to take the mean y value of those, so the output of our model is going to look something like this. And notice it gives the same value at all these points. Eventually, as we query from left to right, we get to a point where this one gets dropped out, and this one gets added in. And at that point we'll have a sudden drop about like that in the model. And we continue on like this. We'll have another drop like that. If we query our model now from left to right in very, very tiny increments we'll get the result that looks something like this. Note that indeed there are sort of jump points here. Some nice things about this are that it's not over fitting the data. In other words, it's not tagging each point. A negative aspect though is at the ends there we have these horizontal lines that are no longer changing or essentially this model is not able to extrapolate like we might if we had a parametric model. Let's consider now what happens to the model that comes out when we change the value of k. So we've got three k nearest neighbor models here. Each one is using a different value of k, and I want you to match the value of k to the output model here. Okay, so I want you to look at these different charts. Each one of these models shown in red is using a different value of k. So I want you to fill in these little boxes, which chart corresponds to the value of k. So one of these charts was created with k=1, one was created with k=3, and another was created with k=N, where N is the total number of elements in the dataset. And there's another question I want you to answer. True or false, as we increase k we are more likely to overfit. I haven't told you yet in too much detail what overfitting is. Let me just give you a quick gist of it so you can answer the question. An overfit model strives really hard to match the dataset exactly. And then when we go on later to use new data or test it with test data, it tends not to do so well. So go for these two questions and I'll come back in a minute and tell you the answer. Alright, let's start with this one. One of these models was created with k=N, and it's this one. If we use all of the neighbors and all the data points, and take their average, the value of our model will be the same at every single point. Namely the mean of all the Y's of all the data points. So this one is b, as in bravo. Let's do this one next, K=1. We know that this model is going to tag each point exactly, because, when we're at that data point, we'll have exactly that value. So this model steps up and down and tags each individual point exactly. So that's a C, and this is, this one is K=3, which we already looked at, A. Okay this next question, as we increase k we are more likely to overfit, that is false. In K and N, if K is equal to one, will have the most overfit model. And as we increase K, we're less and less likely to overfit as we go forward. Let's consider now a similar question, but now we're using parametric models, a polynomial model of degree d. Real quick, here's what we mean when we say polynomial of degree d. So here's what our polynomial model looks like, it's m1 times x, m2 times x squared, m3 times x cubed plus b. This is a third order polynomial, or a polynomial of degree d so I want you to consider d=1, d=2, d=3 and I want you to select which model over here goes with that degree. Then I want you to consider this question. True or false, as we increase d we are more likely to overfit. Okay, let's start with d=1. Well, that's a linear model. That includes just this component. So of course, it must be a line, so it's gotta be this one. That means the answer here is c. Now we have order two and order three to choose from. Two is a parabola, so it's including this component as well. This one's a parabola of course. And so the answer to that one is a. And finally that leaves only b, but let's look at why that is. When we have a cubed component, we can get this additional curl in there. Now as you notice, as we increase from order one to order two to order three, we're gradually getting closer and closer to tagging the actual data. So we get to this question, as we increase d we are more likely to overfit. That is true. And in fact, it can be shown with a polynomial like this that as the order of the polynomial or d reaches in, the total number of points, we actually can match the data at every point. Now a couple things to note here. One is as we go off the edge here for all these models, we're able to extrapolate in the direction the data seem to be going. And this is capability that parametric models or these polynomial models have that KNN does not. I've shown you some graphs that suggest the ways the models can fit the data, more or less closely. But let's have a more formal definition of this matching. It's called error. A standard way to measure error, is called RMS error. Let me show you how to calculate this. Let's suppose we use this data, which are these green points, to build a model. Let's say it's a linear model like this. We can assess the model at each real data point. For instance, at this data point. And measure the difference between the Y value of the data point, and the model. And this difference is error. Now, we've got an error at every single one of these data points. And what we do to measure root mean squared error, is to take the error at each one of these points, square it, add them together, take the average, and take the square root of that. So that sounds kind of complicated, but here's what it looks like. Ytest minus Ypredict. So Ytest are the actual values of the data. Ypredict are what our model predicted. We take that difference at each point. That's this difference. Square it, sum all those together, divide by the number of points and take the square root. And that's our root mean squared error. And what this is an approximation of really, is sort of the average error here. But we end up emphasizing larger errors a bit more. Now, we just measured the error of this linear model against our original training data. We know, though, from say, k and n, that we can build models that can fit this training data exactly. So we can have arbitrarily small error against our training set. The more important measure is, what is our error out of sample? So, what out of sample means is we train on our training set, but we test on a separate testing set of data. And, that's going to be different than our training set. So, to measure out of sample error, we look at the error from our testing set, not our training set. So we look at each one of these test points and measure the error for each one of those. So we look at these blue points instead of the green points, plug them into this equation just like before, and that's our out of sample root mean squared error. Suppose we're measuring the error of a model that you built. Which sort of error would you expect to be larger? In sample error, in which we measure the accuracy of our model against the set it was trained on? Or out of sample error, where we measure the error of the model against a new test set that it hasn't seen before? Which is worse? In general, in fact in almost every case I know of, out of sample error is always worse than in sample error. Usually when researchers are evaluating a learning algorithm, they split their data into two chucks. The training chunk, and a testing chunk. Training usually is about 60% of the data, and testing is about 40%. Now if you train and then test on that data, that's one trial and in many cases that's enough, you measured your root means square error and that's an assessment of your algorithm. You might compare it against another algorithm. But one problem researchers sometimes encounter is they don't have enough data to effectively analyze their algorithm. One thing they can do is effectively create more data by slicing it up and running more trials. Here's how that works. So what we can do is we can slice our data into say five different chunks, and then we can train here on 80% of the data, and test on 20%. That's one trial. Then we can switch things up and train on this 80% of the data. And test on that, that's another trial, and so on. I'm sure you see how this is going. We can effectively get five different trials out of this one set of data. Cross validation is a great tool, but the typical usage of it doesn't fit financial data applications well. The reason is that it can permit peeking into the future. So for instance, if our training data is after our test data that means we're seeing the future ahead of our test. Any sort of peeking like this can read to unrealistically optimistic results, so with this sort of data we need to avoid it. One way to avoid this problem is with role forward cross validation. That means our training data is always before our testing data. But we can still have multiple trials just by rolling our data forward, like this and this and this, till we run out of data. Another way to visualize and evaluate the accuracy of a regression algorithm is to look at the relationship between predicted and actual values of our dependant variable Y. Here's what I mean, query our model, the one that we trained on training data with Xtest, our testing data set. The output of that query is a new vector of Y values, Ypredict. So based on this Xtest data our model predicts this Ypredict data. We can now compare what we know to be the correct, or true, data and Ytest with what our prediction was. So this pair would appear somewhere on this chart, say here. So its a value along the horizontal access here is what the prediction was and along the vertical axis was what the ground truth is. So we can plot these pairs all the way through our data. Now, if this scatterplot is arranged in approximately a nice line like this, that means we've got a pretty good prediction algorithm. On the other hand, if they're not aligned so well and they look sort of like a shotgun blast, our learner is not so good. We can measure this property quantitatively using something called correlation. You can use the num pi function corrcoef to measure the correlation between Ytest and Ypredict. You'll get an answer somewhere between -1 and +1. Where +1 means they're strongly correlated, -1 means they're inversely correlated, and 0 means there's essentially no correlation at all between them. One thing to point out here is that correlation isn't the slope of this line. Lots of people think that's what it is. Correlation has to do with how well aligned the points are with the line that we fit. So if it's a nice oval that fits close to that line, we usually have a high correlation. If it's a big round thing we've got poor correlation. I want you to think now about the relationship between RMS error and correlation. And in particular, I'm talking about correlation between our predicted result and the actual result. Do you think that as RMS error increases, correlation would decrease, correlation would increase, or we can't really be sure? So in most cases, in fact almost all cases, as RMS error increases, correlation decreases. So this would be a reasonably correct answer. But it is possible to construct examples where as RMS error increases, correlation might increase. So that also lets you have it correct if you checked we can't be sure either way. I've mentioned overfitting before, but I haven't yet defined it. Before we could define it, and I could give you an example, we needed to have a definition of error. Let me now show you what I mean. Let's consider parameterized polynomial models where we can, one at a time, add additional factors, like x, x squared, x cubed, x to the fourth, and so on. Let's create a graph where we have along the horizontal access degrees of freedom, or d, the degree of our polynomial. And vertically here, we'll have the error of our model. So let's measure error as we increase d on our training set. So when d is smallest, our error is greatest. And as we increase d, our error drops and drops and drops. In other words, we're fitting the data in sample better and better. When finally we get to N, where we have as many parameters in our model as we do have items in our data set, our error gets all the way down to zero. This is in sample error. Now, let's add a similar line for out of sample error. Remember that we expect our out of sample error to always be greater than or equal to in sample error. The curve will look something like this. It'll start out at maximum error, about the same as our in sample line, and as we go down, we begin to diverge like this. Now in this region both our in sample and out of sample errors are still decreasing, but eventually we'll reach a point where our out of sample begins to increase. In fact it may increase strongly. In this area, as we increase degrees of freedom, our in sample error is decreasing, but our out of sample error is increasing. And that's how we define overfitting. This is the region where overfitting is occurring. So, let me state that again. In sample error is decreasing, out of sample error is increasing. And we have those two together, it's over fitting. I want you now to consider overfitting in KNN. So in this case our horizontal access will be K and it could range again from 1 out to N, the number of data points and then the vertical access will be error. I'm going to draw three charts here showing N sample error as a factor of K and out of sample error as a factor of K. And I want you to look at each one of them and consider which one of them you think is the proper representation of what that ought to look like for KNN. Which of these three charts correctly represents the shape that we would expect for out of sample error and in sample error for KNN, as K increases this way and error increases that way. So take a look and go ahead and fill in your answer over here. So the answer is b. This is a little bit tricky because the relationship for k and n and error is a little bit different than it is for polynomial degrees of freedom and error. Remember that as we reduce k down to 1 our in sample error approaches 0. In fact it becomes a 0 when k is equal to 1. And similarly as we decrease k, our other sample error decreases. But at some point it begins to increase. This one is wrong because as we increase k, our error increases. So this is not showing that relationship correctly. And this is just garbage that I threw in there to see if anybody would bite. [LAUGH] Now the region here in which overfitting is occurring is here, because remember, as out of sample error increases, and in sample error is decreasing, that's where overfitting occurs There are a few other factors worth considering when evaluating a learning algorithm, and I've tallied a few of them here. I want you to think about each one of these and select which you think has better performance in that regard, linear regression or KNN. So let's step through them. How much memory do you need in your computer to save the model? How much compute time do you need to train the model? How long does it take to query the model? And finally, how easy is it to add new data to your model? So, again, I want you to check the box according to which one has better performance with regard to these factors. So, in terms of space for saving the model, linear regression is a hands down winner. For instance, if we're learning a third order polynomial, we have to only store four numbers. KNN, on the other hand, requires you to keep all the data, so it could be megabytes or gigabytes of data. So, KNN is bad in this regard. Compute time to train. KNN is much better in this case. In fact, it takes zero time to train KNN. You just stuff the model into a data store and you're done. On the other hand, linear regression has to take all that data, compute over it, to find those parameters. Compute time to query. LinReg wins hands down. All you do is you plug your X in, multiply it out and that's the answer. KNN requires quite a bit of time to query because you have to, among other things, usually do a sort to cross all the data. Ease to add new data. KNN wins that because all you gotta do is just plop it in there, you don't have to do any re-calculation. With linear regression, you have to add the new data and then recompute the factors. Well, that's all for how to assess learning algorithms. I will see you again soon. Thank you."
    # questions = ["what problem did we pose?",
    #              "what does linear regression create?",
    #              "what will we look at in this lesson?",
    #              "what is a standard way to measure error?",
    #              "what is a hands down winner in terms of memory?",
    #              "what is KNN better at?"]
    #
    # # question_context_batch = []
    # #
    # # for question in questions:
    # #     question_context_batch.append((question, context))

    encoding = tokenizer.batch_encode_plus(question_context_batch, max_length=4096, truncation=True,
                                           pad_to_max_length=True, return_tensors="pt")
    input_ids, attention_mask = encoding["input_ids"], encoding["attention_mask"]
    start_scores, end_scores = model(input_ids, attention_mask=attention_mask)

    for index, (start_score, end_score, input_id) in enumerate(zip(start_scores, end_scores, input_ids)):
        max_startscore = torch.argmax(start_score)
        max_endscore = torch.argmax(end_score)
        sent_start_id, sent_end_id = get_sent_id(input_ids[index], max_startscore, max_endscore)
        ans_ids = input_ids[index][sent_start_id: sent_end_id]
        ans_tokens = tokenizer.convert_ids_to_tokens(ans_ids, skip_special_tokens=True)
        answer = tokenizer.convert_tokens_to_string(ans_tokens)
        summary_tuple.append((sent_start_id, answer))
    summary_tuple = sorted(summary_tuple, key=lambda x: x[0])
    summary = ""
    summary_sent_ids = []
    for i in summary_tuple:
        if i[0] not in summary_sent_ids:
            summary_sent_ids.append(i[0])
            summary += i[1]
    return summary


def main(model, tokenizer, output_dir, lessons_dir, questions_dir):
    output_files = [i for i in next(walk(output_dir))[2] if i.endswith('.txt')] #exclude already generated summary files
    lessons_list = [i for i in next(walk(lessons_dir))[2] if i.endswith('.txt')]
    question_filenames = [i for i in next(walk(questions_dir))[2] if i.endswith('.txt') and i in lessons_list and i not in output_files]
    for question_filename in question_filenames:
        # read the lesson txt file into a string
        path = join(lessons_dir, question_filename)  # the same as lesson file name
        file = open(path, "r")
        context = file.readlines()[0]
        file.close()
        # read the question txt file into a list
        path = join(questions_dir, question_filename)
        file = open(path, "r")
        question_list = file.read().strip().split('\n')
        file.close()
        question_context_batch = []
        for question in question_list:
            question_context_batch.append((question, context))
        summary = generate_qa(question_context_batch, model, tokenizer)
        out_path = join(output_dir, question_filename)
        text_file = open(out_path, "w")
        text_file.truncate()
        text_file.write(summary)
        text_file.close()
        print(question_filename, ' done!')



if __name__ == '__main__':
    parser = argparse.ArgumentParser(description='Generate summaries using a fine-tuned QA model')
    parser.add_argument('--output_dir', action="store", default='./data/qa_generated_summaries/')
    parser.add_argument('--lessons_dir', action="store", default='./data/processed_lessons')
    parser.add_argument('--questions_dir', action="store", default='./data/lesson_questions')
    args = parser.parse_args()
    tokenizer = LongformerTokenizer.from_pretrained("valhalla/longformer-base-4096-finetuned-squadv1")
    model = LongformerForQuestionAnswering.from_pretrained("valhalla/longformer-base-4096-finetuned-squadv1")
    main(model, tokenizer, args.output_dir, args.lessons_dir, args.questions_dir)
