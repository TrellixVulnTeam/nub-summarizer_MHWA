{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Results Analysis and Comparison.ipynb",
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyPka/aDo2j3zMG//e7ZO3DF",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sbassam/nub-summarizer/blob/master/Results_Analysis_and_Comparison.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XZyDR2LfN4Iz",
        "colab_type": "text"
      },
      "source": [
        "In this Colab notebook, the goal is to compare several existing summarizers to nub 1.0 and compare the results to get a better sense of its strengths and weaknesses. Since we've already created a validation dataset in `Run Evaluations on Fine-tuned T-5 Summarizer.ipynb`, we first grab the results. Then run the examples through the competitor summarizers, calculate ROUGE scores, and finally compare them all."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GPP8RmBNOTdg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5xdjHsKZQKJ5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Install the summarizer developed by Derek Miller: https://pypi.org/project/bert-extractive-summarizer/ \n",
        "!pip install bert-extractive-summarizer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FZ_PPDxhSkIq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#ROUGE library\n",
        "!pip install rouge-score"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cdlPEBqXbJLr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# transforerms library\n",
        "!pip install transformers\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4r1kkwd_OwBw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# import libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from summarizer import Summarizer\n",
        "from rouge_score import rouge_scorer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SZEVByPGOYji",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "4bc0d6c5-3e7e-4906-9ac5-2e02e22cc2ba"
      },
      "source": [
        "# navigate to the nub 1.0 summarizer's validation results \n",
        "%cd /content/drive/My Drive/summarizer/nub-training-evaluation/result"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/summarizer/nub-training-evaluation/result\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dYUijfMmPsKg",
        "colab_type": "text"
      },
      "source": [
        "## Run the validation through `bert-extractive-summarizer 0.4.2`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bG_6gXGsOtxW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# read in the validation source and targets\n",
        "eval_results = pd.read_csv('eval_results_t5_base.csv')\n",
        "# initialize the model\n",
        "model = Summarizer()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cpgvemxnTPy4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# define the scorer\n",
        "scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
        "# run each example in the validation through the model and store the scores\n",
        "rouge_1_fs = []\n",
        "rouge_2_fs = []\n",
        "rouge_l_fs = []\n",
        "system_summaries = []\n",
        "i = 0\n",
        "for ind, row in eval_results.iterrows():\n",
        "    # overwrite the systems summary\n",
        "    system_summary = ''.join(model(row.full_text, min_length=60))\n",
        "    scores = scorer.score(row.summary, system_summary)\n",
        "    rouge_1_fs.append(scores['rouge1'].fmeasure)\n",
        "    rouge_2_fs.append(scores['rouge2'].fmeasure)\n",
        "    rouge_l_fs.append(scores['rougeL'].fmeasure)\n",
        "    system_summaries.append(system_summary)\n",
        "    i += 1\n",
        "    print(i)\n",
        "    print(scores)\n",
        "    # overwrite the scores\n",
        "eval_results['rouge1_f']=np.array(rouge_1_fs)\n",
        "eval_results['rouge2_f']=np.array(rouge_2_fs)\n",
        "eval_results['rougeL_f']=np.array(rouge_l_fs)\n",
        "eval_results['system_summary']=np.array(system_summaries)\n",
        "eval_results.to_csv('eval_results_bert_extractive_summarizer.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "souDnNWqagdG",
        "colab_type": "text"
      },
      "source": [
        "## Run the validation set through `mrm8488/t5-base-finetuned-summarize-news`\n",
        "This is a T5 model cound on huggingface's model hub. It was developed by Manuel Romero. https://huggingface.co/mrm8488/t5-base-finetuned-summarize-news\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "pZzZ7qNYbA-7",
        "colab": {}
      },
      "source": [
        "# read in the validation source and targets\n",
        "eval_results = pd.read_csv('eval_results_t5_base.csv')\n",
        "# initialize the model\n",
        "from transformers import AutoTokenizer, AutoModelWithLMHead\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"mrm8488/t5-base-finetuned-summarize-news\")\n",
        "\n",
        "model = AutoModelWithLMHead.from_pretrained(\"mrm8488/t5-base-finetuned-summarize-news\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TE7lcdtbbaZD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "def summarize(text, max_length=150):\n",
        "  '''source https://huggingface.co/mrm8488/t5-base-finetuned-summarize-news'''\n",
        "  input_ids = tokenizer.encode(text, return_tensors=\"pt\", add_special_tokens=True)\n",
        "\n",
        "  generated_ids = model.generate(input_ids=input_ids, num_beams=2, max_length=max_length,  repetition_penalty=2.5, length_penalty=1.0, early_stopping=True)\n",
        "\n",
        "  preds = [tokenizer.decode(g, skip_special_tokens=True, clean_up_tokenization_spaces=True) for g in generated_ids]\n",
        "\n",
        "  return preds[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "91p81TkRbA-_",
        "colab": {}
      },
      "source": [
        "# define the scorer\n",
        "scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
        "# run each example in the validation through the model and store the scores\n",
        "# as well as the generated summaries\n",
        "rouge_1_fs = []\n",
        "rouge_2_fs = []\n",
        "rouge_l_fs = []\n",
        "system_summaries = []\n",
        "i = 0\n",
        "for ind, row in eval_results.iterrows():\n",
        "    # overwrite the systems summary\n",
        "    system_summary = summarize(row.full_text)\n",
        "    scores = scorer.score(row.summary, system_summary)\n",
        "    rouge_1_fs.append(scores['rouge1'].fmeasure)\n",
        "    rouge_2_fs.append(scores['rouge2'].fmeasure)\n",
        "    rouge_l_fs.append(scores['rougeL'].fmeasure)\n",
        "    system_summaries.append(system_summary)\n",
        "    i+=1\n",
        "    print(i)\n",
        "    print(scores)\n",
        "# overwrite the scores\n",
        "eval_results['rouge1_f']=np.array(rouge_1_fs)\n",
        "eval_results['rouge2_f']=np.array(rouge_2_fs)\n",
        "eval_results['rougeL_f']=np.array(rouge_l_fs)\n",
        "eval_results['system_summary']=np.array(system_summaries)\n",
        "eval_results.to_csv('eval_results_t5_mrm8488.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b7Xg4lQtWvc0",
        "colab_type": "text"
      },
      "source": [
        "## Comparative Analysis"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DfXSDgd09cKu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# create an empty master df\n",
        "eval_results_all = pd.DataFrame(\n",
        "    columns=['nub_rouge1_f', 't5_base_rouge1_f', 'bert_ext_summ_rouge1_f', 't5_mrm8488_rouge1_f',\n",
        "             'nub_rouge2_f', 't5_base_rouge2_f', 'bert_ext_summ_rouge2_f', 't5_mrm8488_rouge2_f',\n",
        "             'nub_rougeL_f', 't5_base_rougeL_f', 'bert_ext_summ_rougeL_f', 't5_mrm8488_rougeL_f'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b-lJ4hms-x2s",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# read in all the results sets\n",
        "eval_results_nub = pd.read_csv('eval_results_nub.csv')[['rouge1_f', 'rouge2_f', 'rougeL_f']]\n",
        "eval_results_bert_extractive_summarizer = pd.read_csv('eval_results_bert_extractive_summarizer.csv')[['rouge1_f', 'rouge2_f', 'rougeL_f']]\n",
        "eval_results_t5_base = pd.read_csv('eval_results_t5_base.csv')[['rouge1_f', 'rouge2_f', 'rougeL_f']]\n",
        "eval_results_t5_mrm8488 = pd.read_csv('eval_results_t5_mrm8488.csv')[['rouge1_f', 'rouge2_f', 'rougeL_f']]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lzfdyw8w-y6u",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# insert into the master df\n",
        "eval_results_all = pd.concat([eval_results_nub, eval_results_t5_base, eval_results_bert_extractive_summarizer, eval_results_t5_mrm8488], axis=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1-L1FPO2_oF7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "eval_results_all.columns=['nub_rouge1_f', 'nub_rouge2_f', 'nub_rougeL_f',\n",
        "         't5_base_rouge1_f', 't5_base_rouge2_f', 't5_base_rougeL_f',\n",
        "         'bert_ext_summ_rouge1_f', 'bert_ext_summ_rouge2_f', 'bert_ext_summ_rougeL_f',\n",
        "         't5_mrm8488_rouge1_f', 't5_mrm8488_rouge2_f', 't5_mrm8488_rougeL_f']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SuoidHZFA4V0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "means = eval_results_all.mean()\n",
        "errors = eval_results_all.std()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qv3aWdFxCAQr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "labels = ['R-1', 'R-2', 'R-L']\n",
        "nub_means = 100*eval_results_all[['nub_rouge1_f', 'nub_rouge2_f', 'nub_rougeL_f']].mean().round(4)\n",
        "nub_err = 100*eval_results_all[['nub_rouge1_f', 'nub_rouge2_f', 'nub_rougeL_f']].std().round(4)\n",
        "t5base_means = 100*eval_results_all[['t5_base_rouge1_f', 't5_base_rouge2_f', 't5_base_rougeL_f']].mean().round(4)\n",
        "t5base_err = 100*eval_results_all[['t5_base_rouge1_f', 't5_base_rouge2_f', 't5_base_rougeL_f']].std().round(4)\n",
        "bertextsumm_means = 100*eval_results_all[['bert_ext_summ_rouge1_f', 'bert_ext_summ_rouge2_f', 'bert_ext_summ_rougeL_f']].mean().round(4)\n",
        "bertextsumm_err = 100*eval_results_all[['bert_ext_summ_rouge1_f', 'bert_ext_summ_rouge2_f', 'bert_ext_summ_rougeL_f']].std().round(4)\n",
        "t5mrm8488_means = 100*eval_results_all[['t5_mrm8488_rouge1_f', 't5_mrm8488_rouge2_f', 't5_mrm8488_rougeL_f']].mean().round(4)\n",
        "t5mrm8488_err = 100*eval_results_all[['t5_mrm8488_rouge1_f', 't5_mrm8488_rouge2_f', 't5_mrm8488_rougeL_f']].std().round(4)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zw7KsIoOEljE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# https://matplotlib.org/3.1.1/gallery/lines_bars_and_markers/barchart.html\n",
        "x = np.arange(len(labels))  # the label locations\n",
        "width = 0.2  # the width of the bars\n",
        "\n",
        "fig, ax = plt.subplots()\n",
        "rects1 = ax.bar(x - 3*width/2, nub_means, width, label='Nub 1.0')\n",
        "rects2 = ax.bar(x - width/2, t5base_means, width, label='T5-base')\n",
        "rects3 = ax.bar(x + width/2, bertextsumm_means, width, label='BERT extractive summarizer')\n",
        "rects4 = ax.bar(x + 3*width/2, t5mrm8488_means, width, label='T5 finetuned on Kaggle dataset')\n",
        "\n",
        "# Add some text for labels, title and custom x-axis tick labels, etc.\n",
        "ax.set_ylabel('F-Measure')\n",
        "ax.set_title('ROUGE-1, ROUGE-2 and ROUGE-L for Summarizers')\n",
        "ax.set_xticks(x)\n",
        "ax.set_xticklabels(labels)\n",
        "ax.legend()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "fig.tight_layout()\n",
        "plt.savefig('bar_chart_rouge.png')\n",
        "plt.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rQ8Sh9ygNkCd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "file = open('/content/drive/My Drive/summarizer/t-5-fine-tuned-7-2/resoomer_gold_summary.txt', \"r\")\n",
        "resoomer_gold_summary = ''.join(file.readlines())\n",
        "file.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u3trqoIlNudK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "file = open('/content/drive/My Drive/summarizer/t-5-fine-tuned-7-2/resoomer_validation_output.txt', \"r\")\n",
        "resoomer_system_summary = ''.join(file.readlines())\n",
        "file.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IY1FpFyVIFt1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "resoomer_scores = scorer.score(resoomer_gold_summary, resoomer_system_summary)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NRBJiyfrMo3g",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "66c7b902-e96e-4b01-95ac-5e79281cddd4"
      },
      "source": [
        "resoomer_scores"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'rouge1': Score(precision=0.6404040404040404, recall=0.4283783783783784, fmeasure=0.5133603238866397),\n",
              " 'rouge2': Score(precision=0.151270207852194, recall=0.10117783355860205, fmeasure=0.12125419414555133),\n",
              " 'rougeL': Score(precision=0.14343434343434344, recall=0.09594594594594595, fmeasure=0.11497975708502026)}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 133
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "giYysIY_mD06",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "labels = ['R-1', 'R-2', 'R-L']\n",
        "nub_means = 100*eval_results_all[['nub_rouge1_f', 'nub_rouge2_f', 'nub_rougeL_f']].mean().round(4)\n",
        "t5base_means = 100*eval_results_all[['t5_base_rouge1_f', 't5_base_rouge2_f', 't5_base_rougeL_f']].mean().round(4)\n",
        "bertextsumm_means = 100*eval_results_all[['bert_ext_summ_rouge1_f', 'bert_ext_summ_rouge2_f', 'bert_ext_summ_rougeL_f']].mean().round(4)\n",
        "t5mrm8488_means = 100*eval_results_all[['t5_mrm8488_rouge1_f', 't5_mrm8488_rouge2_f', 't5_mrm8488_rougeL_f']].mean().round(4)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BJDMvjxTmEwM",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "b1aab763-1c40-49bc-8394-7822d41235c3"
      },
      "source": [
        "nub_means"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "nub_rouge1_f    40.79\n",
              "nub_rouge2_f    17.76\n",
              "nub_rougeL_f    26.87\n",
              "dtype: float64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YiSgazsCmF-b",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "c045b14c-bd06-4f89-9d24-c30c292cda35"
      },
      "source": [
        "t5base_means"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "t5_base_rouge1_f    40.19\n",
              "t5_base_rouge2_f    17.35\n",
              "t5_base_rougeL_f    27.65\n",
              "dtype: float64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 145
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DBvg2nsYmOd9",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "cc7dcf4a-9147-4ae5-9397-6f1c59cf80eb"
      },
      "source": [
        "bertextsumm_means"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "bert_ext_summ_rouge1_f    30.89\n",
              "bert_ext_summ_rouge2_f    11.31\n",
              "bert_ext_summ_rougeL_f    18.86\n",
              "dtype: float64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 146
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P0BN_GzumPVu",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "37643972-a555-4fcf-e1ff-8701e1daf320"
      },
      "source": [
        "t5mrm8488_means"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "t5_mrm8488_rouge1_f    37.23\n",
              "t5_mrm8488_rouge2_f    12.98\n",
              "t5_mrm8488_rougeL_f    22.64\n",
              "dtype: float64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 147
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5HCKfN6bmQKu",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "e1a6ba40-b51c-4ab7-fb6f-d3942b23136c"
      },
      "source": [
        "nub_err"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "nub_rouge1_f    11.28\n",
              "nub_rouge2_f    10.09\n",
              "nub_rougeL_f     9.77\n",
              "dtype: float64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nOFShg8dLpMu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}